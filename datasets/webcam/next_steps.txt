What I did:

1. talk about how you generated AOIs - using Oswald's script for CookieTheft, it was easy to create AOI files - generated AOI based features for CookieTheft

2. For Reading, using predicted coordinate files (all-data files used in EMDAT), generated Reading fraser features. This also used Reading AOI files.

3. CookieTheft AOI files are independent of gaze coordinates - they only take segments (which are different for each PID) and AOI files (which are same for all PIDs)

4. In reading task, there were 2 participants which had to be excluded due to issues with my dataset, so the number of participants in CookieTheft and Pupil vs Reading will be different.

5. Fixations and Saccades have been altered - Saccades are no longer generated by the algorithm, instead they fill up the spaces between fixations (any timestamp where there is no fixation, we're assuming a saccade is happening there). This should be closer to the real deal, but the fixation parameters need to be tuned a little bit

Next - 

0. Re-generate pupil features with the new fixation values (Fixation, Saccade, Path) - Re-run for PupilCalib
1. Run ML framework for CookieTheft eye only
2. Run ML framework for Reading eye only
3. Then talk about using speech data to do multimodal ml framework stuff, and slowly build to doing the full Data Ensemble with Averaging (Frontier's results), then compare statistically.





05-April-2022

0. regenerated Pupil features with EMDAT
Ran for PupilCalib - no pupil features - 0.1
1. Ran for CookieTheft eye only - no pupil features - 0.2
2. Ran for Reading eye only - no pupil features - 0.2

3. CANARY: PupilCalib, CookieTheft, Reading are default (with pupil features, 0.2)
